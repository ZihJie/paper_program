{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import requests\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):   \n",
    "    ## chunk 1\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "    prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "    prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "\n",
    "  #############################################################\n",
    "  \n",
    "    for tok in nlp(sent):\n",
    "      ## chunk 2\n",
    "      # if token is a punctuation mark then move on to the next token\n",
    "        if tok.dep_ != \"punct\":\n",
    "          # check: token is a compound word or not\n",
    "            if tok.dep_ == \"compound\":\n",
    "                prefix = tok.text\n",
    "          # if the previous word was also a 'compound' then add the current word to it\n",
    "                if prv_tok_dep == \"compound\":\n",
    "                    prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # check: token is a modifier or not\n",
    "            if tok.dep_.endswith(\"mod\") == True:\n",
    "                modifier = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "                if prv_tok_dep == \"compound\":\n",
    "                    modifier = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      ## chunk 3\n",
    "            if tok.dep_.find(\"subj\") == True:\n",
    "                ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\"      \n",
    "\n",
    "      ## chunk 4\n",
    "            if tok.dep_.find(\"obj\") == True:\n",
    "                ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      ## chunk 5  \n",
    "      # update variables\n",
    "            prv_tok_dep = tok.dep_\n",
    "            prv_tok_text = tok.text\n",
    "  #############################################################\n",
    "\n",
    "    return [ent1.strip(), ent2.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['War', 'Apple']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_entities(\"The War Between Apple and Google Has Just Begun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation(sent):\n",
    "    \n",
    "    doc = nlp(sent)    \n",
    "    \n",
    "    # Matcher class object    \n",
    "    matcher = Matcher(nlp.vocab)  \n",
    "    \n",
    "    #define the pattern    \n",
    "    pattern = [{'DEP':'ROOT'},              \n",
    "               {'DEP':'prep','OP':\"?\"},             \n",
    "               {'DEP':'agent','OP':\"?\"},               \n",
    "               {'POS':'ADJ','OP':\"?\"}]     \n",
    "    \n",
    "    matcher.add(\"matching_1\", None, pattern)    \n",
    "    \n",
    "    matches = matcher(doc)   \n",
    "    k = len(matches) - 1    \n",
    "    \n",
    "    span = doc[matches[k][1]:matches[k][2]]     \n",
    "    \n",
    "    return(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def givemetuple(sent):\n",
    "    print('entity is : ' + str(get_entities(sent)))\n",
    "    print('relation is : ' + str(get_relation(sent)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity is : ['Apple', '300000  Day']\n",
      "relation is : Sold\n"
     ]
    }
   ],
   "source": [
    "givemetuple('Apple Sold 300000 iPads on Day One')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176209it [01:12, 2438.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'stanford-ner-4.2.0.zip'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#下載檔案 Stanford Named Entity Recognizer version 4.2.0\n",
    "\n",
    "import requests as req\n",
    "from tqdm import tqdm\n",
    "url = 'https://nlp.stanford.edu/software/stanford-ner-4.2.0.zip'\n",
    "def download(url):\n",
    "    filename = url.split('/')[-1]\n",
    "    r = req.get(url, stream=True)\n",
    "    with open(filename, 'wb') as f:\n",
    "        for data in tqdm(r.iter_content(1024)):\n",
    "            f.write(data)\n",
    "    return filename\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解壓縮zip檔\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "\n",
    "# zipfile example\n",
    "def zip_list(file_path):\n",
    "    zf = zipfile.ZipFile(file_path, 'r')\n",
    "    zf.extractall()\n",
    "\n",
    "\n",
    "\n",
    "file_path = 'stanford-ner-4.2.0.zip'\n",
    "zip_list(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'ORGANIZATION'), (\"'s\", 'O'), ('rivals', 'O'), ('hope', 'O'), ('its', 'O'), ('iWatch', 'O'), ('makes', 'O'), ('wearable', 'O'), ('work', 'O')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "\n",
    "sentence = u\"Apple's rivals hope its iWatch makes wearable work\"\n",
    "jar = './stanford-ner-tagger/stanford-ner.jar'\n",
    "model = './stanford-ner-tagger/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "\n",
    "# Prepare NER tagger with english model\n",
    "ner_tagger = StanfordNERTagger(model, jar, encoding='utf8')\n",
    "\n",
    "# Tokenize: Split sentence into words\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Run NER tagger on words\n",
    "print(ner_tagger.tag(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Apple Sold 300000 iPads on Day One..\n",
      "Starting server with command: java -Xmx8G -cp /home/zihjie/stanfordnlp_resources/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-c9e5818ec1604062.props -preload openie\n"
     ]
    }
   ],
   "source": [
    "# (範例)用openIE來NER for 單一句子\n",
    "\n",
    "from openie import StanfordOpenIE\n",
    "\n",
    "with StanfordOpenIE()as client:\n",
    "    \n",
    "    text = \"Apple Sold 300000 iPads on Day One.\"\n",
    "    print('Text: %s.' % text)\n",
    "    for triple in client.annotate(text):\n",
    "       # print('|-', triple)\n",
    "        print('subject is : '+triple['subject'])\n",
    "        print('relation is : '+triple['relation'])\n",
    "        print('object is : '+triple['object'])\n",
    "        \n",
    "        print(triple)\n",
    "\n",
    "        print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (範例)用openIE來NER for 一個文件\n",
    "\n",
    "from openie import StanfordOpenIE\n",
    "\n",
    "with StanfordOpenIE()as client:    \n",
    "    with open('corpus_test.txt', 'r', encoding='utf8') as r:\n",
    "        corpuss = r.readlines()\n",
    "    \n",
    "    for corpus in corpuss:\n",
    "        print('corpus is : ' + corpus)\n",
    "        \n",
    "        triples_corpus = client.annotate(corpus[11:])     \n",
    "        print('Corpus: %s [...].' % corpus[11:])     \n",
    "        print('Found %s triples in the corpus.' % len(triples_corpus))  \n",
    "        print('corpus date is ' + corpus[0:11])\n",
    "        for triple in triples_corpus:\n",
    "            print('|-', triple)\n",
    "        \n",
    "        print('-'*50)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (跑workday)用openIE來NER for 一個文件\n",
    "\n",
    "from openie import StanfordOpenIE\n",
    "\n",
    "with StanfordOpenIE()as client:    \n",
    "    with open('News_workday.txt', 'r', encoding='utf8') as r:\n",
    "        corpuss = r.readlines()\n",
    "    \n",
    "    f = open('News_tuple.txt','a')\n",
    "    \n",
    "    for corpus in corpuss:\n",
    "        date = corpus[:10]\n",
    "        triples_corpus = client.annotate(corpus[11:])     \n",
    "        print('Corpus: %s.' % corpus[11:])     \n",
    "        print('Found %s triples in the corpus.' % len(triples_corpus))  \n",
    "        for triple in triples_corpus:\n",
    "            f.write(date +\"    \"+ triple['subject'] +\"    \"+ triple['relation'] +\"    \"+ triple['object'] + \"\\n\")\n",
    "        \n",
    "        print('finish day is : ' + corpus[:10])\n",
    "        \n",
    "        print('-'*50)\n",
    "    \n",
    "    f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/zihjie/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag[0] is : Apple    tag[1] is : NNP\n",
      "wordnet_pos is : n\n",
      "tag[0] is : buys    tag[1] is : VBZ\n",
      "wordnet_pos is : v\n",
      "tag[0] is : Quattro    tag[1] is : NNP\n",
      "wordnet_pos is : n\n",
      "['Apple', 'buy', 'Quattro']\n"
     ]
    }
   ],
   "source": [
    "#詞性還原的範例\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# 獲取單詞的詞性\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "sentence = ' Apple buys Quattro'\n",
    "tokens = word_tokenize(sentence)  # 分詞\n",
    "tagged_sent = pos_tag(tokens)     # 獲取單詞詞性\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmas_sent = []\n",
    "for tag in tagged_sent:\n",
    "    print('tag[0] is : '+tag[0]+'    tag[1] is : '+tag[1] )\n",
    "    wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "    print('wordnet_pos is : '+wordnet_pos)\n",
    "    lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos)) # 詞形還原\n",
    "\n",
    "print(lemmas_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (改良範例)用openIE來NER for 一個文件 use lemmazatization\n",
    "\n",
    "from openie import StanfordOpenIE\n",
    "\n",
    "with StanfordOpenIE()as client:    \n",
    "    with open('corpus_test.txt', 'r', encoding='utf8') as r:\n",
    "        corpuss = r.readlines()\n",
    "    \n",
    "    for corpus in corpuss:\n",
    "        print('corpus is : ' + corpus)\n",
    "        \n",
    "        sentence =  str(corpus[11:]).capitalize()\n",
    "        \n",
    "        tokens = word_tokenize(sentence) # 分詞\n",
    "        tagged_sent = pos_tag(tokens)      # 獲取單詞詞性\n",
    "        \n",
    "        wnl = WordNetLemmatizer()\n",
    "        lemmas_sent = []\n",
    "        for tag in tagged_sent:\n",
    "            wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "            lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos)) # 詞形還原\n",
    "        \n",
    "        sen_corpus = ' '.join(lemmas_sent)\n",
    "        \n",
    "        print('after sentence is : ' + sen_corpus)\n",
    "        \n",
    "        \n",
    "        triples_corpus = client.annotate(sen_corpus)     \n",
    "        print('Corpus: %s [...].' % sen_corpus)\n",
    "        print('Found %s triples in the corpus.' % len(sen_corpus))  \n",
    "        print('corpus date is ' + corpus[0:11])\n",
    "        \n",
    "        for triple in triples_corpus:\n",
    "            print('triple is : ' + triple['subject'] +\"    \"+ triple['relation'] +\"    \"+ triple['object'] + \"\\n\")    \n",
    "        print('-'*50)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正式開始做詞性轉換並獲得三元組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (改良範例)用openIE來NER for 一個文件 use lemmazatization\n",
    "\n",
    "from openie import StanfordOpenIE\n",
    "\n",
    "with StanfordOpenIE()as client:    \n",
    "    with open('News_workday.txt', 'r', encoding='utf8') as r:\n",
    "        corpuss = r.readlines()\n",
    "    \n",
    "    #開啟要寫的文件\n",
    "    f = open('News_tuple_ex.txt','a')\n",
    "    \n",
    "    for corpus in corpuss:\n",
    "        print('originally sentence is : ' + corpus)\n",
    "        \n",
    "        sentence =  str(corpus[11:]).capitalize()\n",
    "        \n",
    "        tokens = word_tokenize(sentence) # 分詞\n",
    "        tagged_sent = pos_tag(tokens)    # 獲取單詞詞性\n",
    "        \n",
    "        wnl = WordNetLemmatizer()\n",
    "        lemmas_sent = []\n",
    "        for tag in tagged_sent:\n",
    "            wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "            lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos)) # 詞形還原\n",
    "        \n",
    "        #把詞形還原後的詞接再一起變句子\n",
    "        sen_corpus = ' '.join(lemmas_sent)\n",
    "        \n",
    "        print('after sentence is : ' + sen_corpus)\n",
    "        \n",
    "        #將句子拆成三元組\n",
    "        triples_corpus = client.annotate(sen_corpus)     \n",
    "        print('Corpus: %s [...].' % sen_corpus)\n",
    "        print('Found %s triples in the corpus.' % len(sen_corpus))  \n",
    "        print('corpus date is ' + corpus[0:10])\n",
    "        \n",
    "        for triple in triples_corpus:\n",
    "            print('triple is : ' + triple['subject'] +\"    \"+ triple['relation'] +\"    \"+ triple['object'] + \"\\n\")    \n",
    "            f.write(corpus[:10] +\"    \"+ triple['subject'] +\"    \"+ triple['relation'] +\"    \"+ triple['object'] + \"\\n\")\n",
    "        \n",
    "        print('-'*50)\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將三元組分成等等要用的訓練檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's finish~\n"
     ]
    }
   ],
   "source": [
    "f = open('Workday_tuple_ex.txt','r')\n",
    "\n",
    "#entity和relation部分在下一段落做\n",
    "#e = open('entity2id.txt','a')\n",
    "#r = open('relation2id.txt','a')\n",
    "\n",
    "t = open('train.txt','a')\n",
    "\n",
    "head = []\n",
    "relation  = []\n",
    "tail = []\n",
    "\n",
    "entity = []\n",
    "trainfile = []\n",
    "\n",
    "for lines in f :\n",
    "    triple = lines[15:].replace('\\n','').lower().split(\"-----\")\n",
    "    #print(triple)\n",
    "    \n",
    "    head.append(triple[0])\n",
    "    relation.append(triple[1])\n",
    "    tail.append(triple[2])\n",
    "    \n",
    "    entity.append(triple[0])\n",
    "    entity.append(triple[2])\n",
    "    \n",
    "    #到時候write直接寫上去即可\n",
    "    #trainfile.append(triple[0]+\"\\t\"+triple[2]+\"\\t\"+triple[1])\n",
    "    #train的部分\n",
    "    #t.write(triple[0] + \"\\t\" + triple[2] + \"\\t\" + triple[1] + \"\\n\")\n",
    "    \n",
    "    #print('-'*50)\n",
    "\n",
    "print(\"It's finish~\")\n",
    "\n",
    "f.close()\n",
    "t.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entity2id部分\n",
    "\n",
    "e = open('entity_noorder.txt','a')\n",
    "r = open('relation_noorder.txt','a')\n",
    "\n",
    "for index,subject in enumerate(list({}.fromkeys(entity).keys())):\n",
    "    #print(subject, index)\n",
    "    e.write(subject  + \"\\n\")\n",
    "e.close()\n",
    "\n",
    "for index,rela in enumerate(list({}.fromkeys(relation).keys())):\n",
    "    r.write(rela  + \"\\n\")\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's finish~\n"
     ]
    }
   ],
   "source": [
    "#word2vec所需訓練之文件\n",
    "\n",
    "#讀取資料來源\n",
    "f = open('Workday_tuple_ex.txt','r')\n",
    "#要寫入的資料，之後來訓練word2vec\n",
    "w = open('word2vec_corpus.txt','a')\n",
    "\n",
    "head = []\n",
    "relation  = []\n",
    "tail = []\n",
    "\n",
    "entity = []\n",
    "trainfile = []\n",
    "\n",
    "for lines in f :\n",
    "    triple = lines[15:].replace('\\n','').lower().split(\"-----\")\n",
    "    #print(triple)\n",
    "    \n",
    "    head.append(triple[0])\n",
    "    relation.append(triple[1])\n",
    "    tail.append(triple[2])\n",
    "    \n",
    "    w.write(\"[\" +\"'\" + triple[0] + \"'\"+\",\"+\"'\" + triple[1] + \"'\"+\",\" +\"'\"+ triple[2] +\"'\"+ \"],\" )\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "print(\"It's finish~\")\n",
    "\n",
    "f.close()\n",
    "w.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransE的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import uniform, sample, choice\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def get_details_of_entityOrRels_list(file_path, split_delimeter=\"\\t\"):\n",
    "    num_of_file = 0\n",
    "    lyst = []\n",
    "    with open(file_path) as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            details_and_id = line.strip().split(split_delimeter)\n",
    "            lyst.append(details_and_id[0])\n",
    "            num_of_file += 1\n",
    "    return num_of_file, lyst\n",
    "\n",
    "\n",
    "def get_details_of_triplets_list(file_path, split_delimeter=\"\\t\"):\n",
    "    num_of_file = 0\n",
    "    lyst = []\n",
    "    with open(file_path) as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            triple = line.strip().split(split_delimeter)\n",
    "            if len(triple) < 3:\n",
    "                continue\n",
    "            lyst.append(tuple(triple))\n",
    "            num_of_file += 1\n",
    "    return num_of_file, lyst\n",
    "\n",
    "\n",
    "def norm(lyst):\n",
    "    # 归一化 单位向量\n",
    "    var = np.linalg.norm(lyst)\n",
    "    i = 0\n",
    "    while i < len(lyst):\n",
    "        lyst[i] = lyst[i] / var\n",
    "        i += 1\n",
    "    # 需要返回array值 因为list不支持减法\n",
    "    # return list\n",
    "    return np.array(lyst)\n",
    "\n",
    "\n",
    "def dist_L1(h, t, l):\n",
    "    s = h + l - t\n",
    "    # 曼哈顿距离/出租车距离， |x-xi|+|y-yi|直接对向量的各个维度取绝对值相加\n",
    "    # dist = np.fabs(s).sum()\n",
    "    return np.fabs(s).sum()\n",
    "\n",
    "\n",
    "def dist_L2(h, t, l):\n",
    "    s = h + l - t\n",
    "    # 欧氏距离,是向量的平方和未开方。一定要注意，归一化公式和距离公式的错误书写，会引起收敛的失败\n",
    "    # dist = (s * s).sum()\n",
    "    return (s * s).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransE(object):\n",
    "    def __init__(self, entity_list, rels_list, triplets_list, margin=1, learing_rate=0.01, dim=20, normal_form=\"L1\"):\n",
    "        self.learning_rate = learing_rate\n",
    "        self.loss = 0\n",
    "        self.entity_list = entity_list  # entityList是entity的list；初始化后，变为字典，key是entity，values是其向量（使用narray）。\n",
    "        self.rels_list = rels_list\n",
    "        self.triplets_list = triplets_list\n",
    "        self.margin = margin\n",
    "        self.dim = dim\n",
    "        self.normal_form = normal_form\n",
    "        self.entity_vector_dict = {}\n",
    "        self.rels_vector_dict = {}\n",
    "        self.loss_list = []\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"对论文中的初始化稍加改动\n",
    "        初始化l和e，对于原本的l和e的文件中的/m/06rf7字符串标识转化为定义的dim维向量，对dim维向量进行uniform和norm归一化操作\n",
    "        \"\"\"\n",
    "        entity_vector_dict, rels_vector_dict = {}, {}\n",
    "        entity_vector_compo_list, rels_vector_compo_list = [], []\n",
    "        for item, dict, compo_list, name in zip(\n",
    "                [self.entity_list, self.rels_list], [entity_vector_dict, rels_vector_dict],\n",
    "                [entity_vector_compo_list, rels_vector_compo_list], [\"entity_vector_dict\", \"rels_vector_dict\"]):\n",
    "            for entity_or_rel in item:\n",
    "                n = 0\n",
    "                compo_list = []\n",
    "                while n < self.dim:\n",
    "                    random = uniform(-6 / (self.dim ** 0.5), 6 / (self.dim ** 0.5))\n",
    "                    compo_list.append(random)\n",
    "                    n += 1\n",
    "                compo_list = norm(compo_list)\n",
    "                dict[entity_or_rel] = compo_list\n",
    "            print(\"The \" + name + \"'s initialization is over. It's number is %d.\" % len(dict))\n",
    "        self.entity_vector_dict = entity_vector_dict\n",
    "        self.rels_vector_dict = rels_vector_dict\n",
    "\n",
    "    def transE(self, cycle_index=20):\n",
    "        print(\"\\n********** Start TransE training **********\")\n",
    "        for i in range(cycle_index):\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(\"----------------The {} batchs----------------\".format(i))\n",
    "                print(\"The loss is: %.4f\" % self.loss)\n",
    "                # 查看最后的结果收敛情况\n",
    "                self.loss_list.append(self.loss)\n",
    "                # self.write_vector(\"data/entityVector.txt\", \"entity\")\n",
    "                # self.write_vector(\"data/relationVector.txt\", \"rels\")\n",
    "                self.loss = 0\n",
    "\n",
    "            Sbatch = self.sample(150)\n",
    "            Tbatch = []  # 元组对（原三元组，打碎的三元组）的列表 ：{((h,r,t),(h',r,t'))}\n",
    "            for sbatch in Sbatch:\n",
    "                triplets_with_corrupted_triplets = (sbatch, self.get_corrupted_triplets(sbatch))\n",
    "                if triplets_with_corrupted_triplets not in Tbatch:\n",
    "                    Tbatch.append(triplets_with_corrupted_triplets)\n",
    "            self.update(Tbatch)\n",
    "\n",
    "    def sample(self, size):\n",
    "        return sample(self.triplets_list, size)\n",
    "\n",
    "    def get_corrupted_triplets(self, triplets):\n",
    "        '''training triplets with either the head or tail replaced by a random entity (but not both at the same time)\n",
    "        :param triplet:单个（h,t,l）\n",
    "        :return corruptedTriplet:'''\n",
    "        # i = uniform(-1, 1) if i\n",
    "        coin = choice([True, False])\n",
    "        # 由于这个时候的(h,t,l)是从train文件里面抽出来的，要打坏的话直接随机寻找一个和头实体不等的实体即可\n",
    "        if coin:  # 抛硬币 为真 打破头实体，即第一项\n",
    "            while True:\n",
    "                searching_entity = sample(self.entity_vector_dict.keys(), 1)[0]  # 取第一个元素是因为sample返回的是一个列表类型\n",
    "                if searching_entity != triplets[0]:\n",
    "                    break\n",
    "            corrupted_triplets = (searching_entity, triplets[1], triplets[2])\n",
    "        else:  # 反之，打破尾实体，即第二项\n",
    "            while True:\n",
    "                searching_entity = sample(self.entity_vector_dict.keys(), 1)[0]\n",
    "                if searching_entity != triplets[1]:\n",
    "                    break\n",
    "            corrupted_triplets = (triplets[0], searching_entity, triplets[2])\n",
    "        return corrupted_triplets\n",
    "\n",
    "    def update(self, Tbatch):\n",
    "        entity_vector_copy = deepcopy(self.entity_vector_dict)\n",
    "        rels_vector_copy = deepcopy(self.rels_vector_dict)\n",
    "        #print(entity_vector_copy)\n",
    "\n",
    "        for triplets_with_corrupted_triplets in Tbatch:\n",
    "            head_entity_vector = entity_vector_copy[triplets_with_corrupted_triplets[0][0]]\n",
    "            tail_entity_vector = entity_vector_copy[triplets_with_corrupted_triplets[0][1]]\n",
    "            relation_vector = rels_vector_copy[triplets_with_corrupted_triplets[0][2]]\n",
    "\n",
    "            head_entity_vector_with_corrupted_triplets = entity_vector_copy[triplets_with_corrupted_triplets[1][0]]\n",
    "            tail_entity_vector_with_corrupted_triplets = entity_vector_copy[triplets_with_corrupted_triplets[1][1]]\n",
    "\n",
    "            head_entity_vector_before_batch = self.entity_vector_dict[triplets_with_corrupted_triplets[0][0]]\n",
    "            tail_entity_vector_before_batch = self.entity_vector_dict[triplets_with_corrupted_triplets[0][1]]\n",
    "            relation_vector_before_batch = self.rels_vector_dict[triplets_with_corrupted_triplets[0][2]]\n",
    "\n",
    "            head_entity_vector_with_corrupted_triplets_before_batch = self.entity_vector_dict[\n",
    "                triplets_with_corrupted_triplets[1][0]]\n",
    "            tail_entity_vector_with_corrupted_triplets_before_batch = self.entity_vector_dict[\n",
    "                triplets_with_corrupted_triplets[1][1]]\n",
    "\n",
    "            if self.normal_form == \"L1\":\n",
    "                dist_triplets = dist_L1(head_entity_vector_before_batch, tail_entity_vector_before_batch,\n",
    "                                        relation_vector_before_batch)\n",
    "                dist_corrupted_triplets = dist_L1(head_entity_vector_with_corrupted_triplets_before_batch,\n",
    "                                                  tail_entity_vector_with_corrupted_triplets_before_batch,\n",
    "                                                  relation_vector_before_batch)\n",
    "            else:\n",
    "                dist_triplets = dist_L2(head_entity_vector_before_batch, tail_entity_vector_before_batch,\n",
    "                                        relation_vector_before_batch)\n",
    "                dist_corrupted_triplets = dist_L2(head_entity_vector_with_corrupted_triplets_before_batch,\n",
    "                                                  tail_entity_vector_with_corrupted_triplets_before_batch,\n",
    "                                                  relation_vector_before_batch)\n",
    "            eg = self.margin + dist_triplets - dist_corrupted_triplets\n",
    "            if eg > 0:  # 大于0取原值，小于0则置0.即合页损失函数margin-based ranking criterion\n",
    "                self.loss += eg\n",
    "                temp_positive = 2 * self.learning_rate * (\n",
    "                        tail_entity_vector_before_batch - head_entity_vector_before_batch - relation_vector_before_batch)\n",
    "                temp_negative = 2 * self.learning_rate * (\n",
    "                        tail_entity_vector_with_corrupted_triplets_before_batch - head_entity_vector_with_corrupted_triplets_before_batch - relation_vector_before_batch)\n",
    "                if self.normal_form == \"L1\":\n",
    "                    temp_positive_L1 = [1 if temp_positive[i] >= 0 else -1 for i in range(self.dim)]\n",
    "                    temp_negative_L1 = [1 if temp_negative[i] >= 0 else -1 for i in range(self.dim)]\n",
    "                    temp_positive_L1 = [float(f) for f in temp_positive_L1]\n",
    "                    temp_negative_L1 = [float(f) for f in temp_negative_L1]\n",
    "                    temp_positive = np.array(temp_positive_L1) * self.learning_rate\n",
    "                    temp_negative = np.array(temp_negative_L1) * self.learning_rate\n",
    "                    # temp_positive = norm(temp_positive_L1) * self.learning_rate\n",
    "                    # temp_negative = norm(temp_negative_L1) * self.learning_rate\n",
    "\n",
    "                # 对损失函数的5个参数进行梯度下降， 随机体现在sample函数上\n",
    "                head_entity_vector += temp_positive\n",
    "                tail_entity_vector -= temp_positive\n",
    "                relation_vector = relation_vector + temp_positive - temp_negative\n",
    "                head_entity_vector_with_corrupted_triplets -= temp_negative\n",
    "                tail_entity_vector_with_corrupted_triplets += temp_negative\n",
    "\n",
    "                # 归一化刚才更新的向量，减少计算时间\n",
    "                entity_vector_copy[triplets_with_corrupted_triplets[0][0]] = norm(head_entity_vector)\n",
    "                entity_vector_copy[triplets_with_corrupted_triplets[0][1]] = norm(tail_entity_vector)\n",
    "                rels_vector_copy[triplets_with_corrupted_triplets[0][2]] = norm(relation_vector)\n",
    "                entity_vector_copy[triplets_with_corrupted_triplets[1][0]] = norm(\n",
    "                    head_entity_vector_with_corrupted_triplets)\n",
    "                entity_vector_copy[triplets_with_corrupted_triplets[1][1]] = norm(\n",
    "                    tail_entity_vector_with_corrupted_triplets)\n",
    "\n",
    "                # self.entity_vector_dict = deepcopy(entity_vector_copy)\n",
    "                # self.rels_vector_dict = deepcopy(rels_vector_copy)\n",
    "            self.entity_vector_dict = entity_vector_copy\n",
    "            self.rels_vector_dict = rels_vector_copy\n",
    "\n",
    "    def write_vector(self, file_path, option):\n",
    "        if option.strip().startswith(\"entit\"):\n",
    "            print(\"Write entities vetor into file      : {}\".format(file_path))\n",
    "            # dyct = deepcopy(self.entity_vector_dict)\n",
    "            dyct = self.entity_vector_dict\n",
    "        if option.strip().startswith(\"rel\"):\n",
    "            print(\"Write relationships vector into file: {}\".format(file_path))\n",
    "            # dyct = deepcopy(self.rels_vector_dict)\n",
    "            dyct = self.rels_vector_dict\n",
    "        with open(file_path, 'w') as file:  # 写文件，每次覆盖写 用with自动调用close\n",
    "            for dyct_key in dyct.keys():\n",
    "                file.write(dyct_key + \"\\t\")\n",
    "                file.write(str(dyct[dyct_key].tolist()))\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "    def write_loss(self, file_path, num_of_col):\n",
    "        with open(file_path, 'w') as file:\n",
    "            lyst = deepcopy(self.loss_list)\n",
    "            for i in range(len(lyst)):\n",
    "                if num_of_col == 1:\n",
    "                    # 保留4位小数\n",
    "                    file.write(str(int(lyst[i] * 10000) / 10000) + \"\\n\")\n",
    "                    # file.write(str(lyst[i]).split('.')[0] + '.' + str(lyst[i]).split('.')[1][:4] + \"\\n\")\n",
    "                else:\n",
    "                    # file.write(str(lyst[i]).split('.')[0] + '.' + str(lyst[i]).split('.')[1][:4] + \"\\t\")\n",
    "                    file.write(str(int(lyst[i] * 10000) / 10000) + \"    \")\n",
    "                    if (i + 1) % num_of_col == 0 and i != 0:\n",
    "                        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TransE is initializing...\n",
      "The entity_vector_dict's initialization is over. It's number is 1366.\n",
      "The rels_vector_dict's initialization is over. It's number is 676.\n",
      "\n",
      "********** Start TransE training **********\n",
      "----------------The 0 batchs----------------\n",
      "The loss is: 0.0000\n",
      "----------------The 100 batchs----------------\n",
      "The loss is: 4186.5572\n",
      "----------------The 200 batchs----------------\n",
      "The loss is: 485.5670\n",
      "----------------The 300 batchs----------------\n",
      "The loss is: 250.9241\n",
      "----------------The 400 batchs----------------\n",
      "The loss is: 154.5309\n",
      "----------------The 500 batchs----------------\n",
      "The loss is: 126.7138\n",
      "----------------The 600 batchs----------------\n",
      "The loss is: 98.0635\n",
      "----------------The 700 batchs----------------\n",
      "The loss is: 64.2522\n",
      "----------------The 800 batchs----------------\n",
      "The loss is: 65.9256\n",
      "----------------The 900 batchs----------------\n",
      "The loss is: 46.6238\n",
      "----------------The 1000 batchs----------------\n",
      "The loss is: 43.9631\n",
      "----------------The 1100 batchs----------------\n",
      "The loss is: 41.9384\n",
      "----------------The 1200 batchs----------------\n",
      "The loss is: 27.2112\n",
      "----------------The 1300 batchs----------------\n",
      "The loss is: 39.1626\n",
      "----------------The 1400 batchs----------------\n",
      "The loss is: 35.2974\n",
      "----------------The 1500 batchs----------------\n",
      "The loss is: 28.8844\n",
      "----------------The 1600 batchs----------------\n",
      "The loss is: 35.9161\n",
      "----------------The 1700 batchs----------------\n",
      "The loss is: 29.5128\n",
      "----------------The 1800 batchs----------------\n",
      "The loss is: 37.7190\n",
      "----------------The 1900 batchs----------------\n",
      "The loss is: 41.5201\n",
      "----------------The 2000 batchs----------------\n",
      "The loss is: 28.0418\n",
      "----------------The 2100 batchs----------------\n",
      "The loss is: 32.6040\n",
      "----------------The 2200 batchs----------------\n",
      "The loss is: 20.1209\n",
      "----------------The 2300 batchs----------------\n",
      "The loss is: 23.6168\n",
      "----------------The 2400 batchs----------------\n",
      "The loss is: 24.3368\n",
      "----------------The 2500 batchs----------------\n",
      "The loss is: 26.7663\n",
      "----------------The 2600 batchs----------------\n",
      "The loss is: 14.8635\n",
      "----------------The 2700 batchs----------------\n",
      "The loss is: 23.2517\n",
      "----------------The 2800 batchs----------------\n",
      "The loss is: 16.4319\n",
      "----------------The 2900 batchs----------------\n",
      "The loss is: 23.8198\n",
      "----------------The 3000 batchs----------------\n",
      "The loss is: 18.7280\n",
      "----------------The 3100 batchs----------------\n",
      "The loss is: 14.6339\n",
      "----------------The 3200 batchs----------------\n",
      "The loss is: 18.0559\n",
      "----------------The 3300 batchs----------------\n",
      "The loss is: 21.7811\n",
      "----------------The 3400 batchs----------------\n",
      "The loss is: 14.6942\n",
      "----------------The 3500 batchs----------------\n",
      "The loss is: 15.5875\n",
      "----------------The 3600 batchs----------------\n",
      "The loss is: 17.3567\n",
      "----------------The 3700 batchs----------------\n",
      "The loss is: 14.4760\n",
      "----------------The 3800 batchs----------------\n",
      "The loss is: 13.2818\n",
      "----------------The 3900 batchs----------------\n",
      "The loss is: 15.5189\n",
      "----------------The 4000 batchs----------------\n",
      "The loss is: 18.0988\n",
      "----------------The 4100 batchs----------------\n",
      "The loss is: 23.4895\n",
      "----------------The 4200 batchs----------------\n",
      "The loss is: 15.5433\n",
      "----------------The 4300 batchs----------------\n",
      "The loss is: 16.2461\n",
      "----------------The 4400 batchs----------------\n",
      "The loss is: 14.8803\n",
      "----------------The 4500 batchs----------------\n",
      "The loss is: 12.7569\n",
      "----------------The 4600 batchs----------------\n",
      "The loss is: 9.2998\n",
      "----------------The 4700 batchs----------------\n",
      "The loss is: 11.2688\n",
      "----------------The 4800 batchs----------------\n",
      "The loss is: 12.1417\n",
      "----------------The 4900 batchs----------------\n",
      "The loss is: 15.2371\n",
      "********** End TransE training ***********\n",
      "\n",
      "Write entities vetor into file      : ./entityVector(20d).txt\n",
      "Write relationships vector into file: ./relationVector(20d).txt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    entity_file_path = \"./entity2id.txt\"\n",
    "    num_of_entity, entity_list = get_details_of_entityOrRels_list(entity_file_path)\n",
    "    rels_file_path = \"./relation2id.txt\"\n",
    "    num_of_rels, rels_list = get_details_of_entityOrRels_list(rels_file_path)\n",
    "    train_file_path = \"./train.txt\"\n",
    "    num_of_triplets, triplets_list = get_details_of_triplets_list(train_file_path)\n",
    "\n",
    "    transE = TransE(entity_list, rels_list, triplets_list, margin=1, dim=50)\n",
    "    print(\"\\nTransE is initializing...\")\n",
    "    transE.initialize()\n",
    "    transE.transE(5000)\n",
    "    print(\"********** End TransE training ***********\\n\")\n",
    "    # 训练的批次并不一定是100的整数倍，将最后更新的向量写到文件\n",
    "    transE.write_vector(\"./entityVector(20d).txt\", \"entity\")\n",
    "    transE.write_vector(\"./relationVector(20d).txt\", \"relationship\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_of_entity, entity_list = get_details_of_entityOrRels_list(entity_file_path)\n",
    "#num_of_rels, rels_list = get_details_of_entityOrRels_list(rels_file_path)\n",
    "#num_of_triplets, triplets_list = get_details_of_triplets_list(train_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
